{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7826f6e8",
   "metadata": {},
   "source": [
    "Wrapper to train binary class classifier including: LR, DT, RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# ---- Step 1: Load stratified data ----\n",
    "with open('Data/Pre-processed_data/70-30/X_train_enc.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('Data/Pre-processed_data/70-30/X_test_enc.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open('Data/Pre-processed_data/70-30/y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('Data/Pre-processed_data/70-30/y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "# ---- Step 2: Convert target to binary ----\n",
    "y_train_bin = y_train.map({'No': 0, 'Yes': 1})\n",
    "y_test_bin  = y_test.map( {'No': 0, 'Yes': 1})\n",
    "\n",
    "# ---- Step 3: Check for NaNs ----\n",
    "print(\"\\nMissing values in X_train:\")\n",
    "print(X_train.isna().sum()[X_train.isna().sum() > 0])\n",
    "\n",
    "print(\"\\nMissing values in X_test:\")\n",
    "print(X_test.isna().sum()[X_test.isna().sum() > 0])\n",
    "\n",
    "# ---- Step 4: Define models with preprocessing pipeline ----\n",
    "# Create a preprocessing step: impute + scale\n",
    "preprocessing = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Use 'most_frequent' for categorical\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define models wrapped in pipelines\n",
    "models = {\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('classifier', LogisticRegression(class_weight='balanced', random_state=42))\n",
    "    ]),\n",
    "    'RandomForest': Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('classifier', RandomForestClassifier(class_weight='balanced', n_estimators=200, random_state=42))\n",
    "    ]),\n",
    "    'Decision Trees': Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('classifier', DecisionTreeClassifier(criterion=\"gini\", random_state=None))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# ---- Step 5: Train, predict, evaluate, visualize ----\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"\\n{'='*25} {name} {'='*25}\")\n",
    "    pipeline.fit(X_train, y_train_bin)\n",
    "\n",
    "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_pred  = pipeline.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test_bin, y_pred)\n",
    "    prec = precision_score(y_test_bin, y_pred)\n",
    "    rec  = recall_score(y_test_bin, y_pred)\n",
    "    f1   = f1_score(y_test_bin, y_pred)\n",
    "    mcc  = matthews_corrcoef(y_test_bin, y_pred)\n",
    "    ap   = average_precision_score(y_test_bin, y_proba)\n",
    "    roc_auc = roc_auc_score(y_test_bin, y_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_bin, y_proba)\n",
    "\n",
    "    print(f\"ROC-AUC:    {roc_auc:.3f}\")\n",
    "    print(f\"Precision:  {prec:.3f}\")\n",
    "    print(f\"Recall:     {rec:.3f}\")\n",
    "    print(f\"F1-score:   {f1:.3f}\")\n",
    "    print(f\"MCC:        {mcc:.3f}\")\n",
    "    print(f\"AUC-PR:     {ap:.3f}\")\n",
    "    print(f\"Accuracy:    {acc: .5f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test_bin, y_pred, target_names=['No','Yes']))\n",
    "\n",
    "    # ---- Confusion Matrix ----\n",
    "    cm = confusion_matrix(y_test_bin, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "    plt.title(f'{name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- Precision-Recall Curve ----\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test_bin, y_proba)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(recall_vals, precision_vals, label=f'{name} (AUC-PR = {ap:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{name} - Precisionâ€“Recall Curve')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='lower left')\n",
    "\n",
    "    # ---- AUC-ROC Curve ----\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{name} - ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Annotate metrics on plot\n",
    "    plt.text(0.6, 0.2, f'F1-score: {f1:.2f}\\nPrecision: {prec:.2f}\\nRecall: {rec:.2f}',\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', edgecolor='black'))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e05e91",
   "metadata": {},
   "source": [
    "Wrapper to train binary class classifier including: SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef, average_precision_score,\n",
    "    precision_recall_curve, classification_report,\n",
    "    confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# ---- Step 1: Load Data ----\n",
    "with open('Data/Pre-processed_data/70-30/X_train_enc.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('Data/Pre-processed_data/70-30/X_test_enc.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open('Data/Pre-processed_data/70-30/y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('Data/Pre-processed_data/70-30/y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "# ---- Step 2: Impute Missing Values ----\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# ---- Step 3: Scale Features ----\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ---- Step 4: Convert Target to Binary ----\n",
    "y_train_bin = y_train.map({'No': 0, 'Yes': 1})\n",
    "y_test_bin  = y_test.map( {'No': 0, 'Yes': 1})\n",
    "\n",
    "# ---- Step 5: Define Parameter Grid ----\n",
    "param_grid = {\n",
    "    'C': [0.1, 1],\n",
    "    'gamma': ['scale', 0.01, 0.1],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "svc = SVC(probability=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    svc,\n",
    "    param_grid,\n",
    "    scoring='average_precision',  # Better for imbalanced data\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ---- Step 6: Fit Model ----\n",
    "grid_search.fit(X_train, y_train_bin)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nBest Parameters from GridSearch:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# ---- Step 7: Evaluate on Test Data ----\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "y_pred  = best_model.predict(X_test)\n",
    "\n",
    "prec = precision_score(y_test_bin, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_test_bin, y_pred, zero_division=0)\n",
    "f1   = f1_score(y_test_bin, y_pred, zero_division=0)\n",
    "mcc  = matthews_corrcoef(y_test_bin, y_pred)\n",
    "ap   = average_precision_score(y_test_bin, y_proba)\n",
    "acc = accuracy_score(y_test_bin, y_pred)\n",
    "roc_auc = roc_auc_score(y_test_bin, y_proba)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_bin, y_proba)\n",
    "\n",
    "print(\"\\n===== SVC with Grid Search Evaluation =====\")\n",
    "print(f'ROC_AUC :     {roc_auc: .3f}')\n",
    "print(f'Accuracy:    {acc: .5f}')\n",
    "print(f\"Precision:  {prec:.3f}\")\n",
    "print(f\"Recall:     {rec:.3f}\")\n",
    "print(f\"F1-score:   {f1:.3f}\")\n",
    "print(f\"MCC:        {mcc:.3f}\")\n",
    "print(f\"AUC-PR:     {ap:.3f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_bin, y_pred, target_names=['No', 'Yes'], zero_division=0))\n",
    "\n",
    "# ---- Step 8: Confusion Matrix ----\n",
    "cm = confusion_matrix(y_test_bin, y_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('SVC (GridSearch) - Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ---- AUC-ROC Curve ----\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{name} - ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Step 9: Precision-Recall Curve ----\n",
    "precision, recall, _ = precision_recall_curve(y_test_bin, y_proba)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall, precision, label=f'SVC (AUC-PR = {ap:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('SVC - Precisionâ€“Recall Curve')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.text(0.6, 0.2, f'F1-score: {f1:.2f}\\nPrecision: {prec:.2f}\\nRecall: {rec:.2f}',\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', edgecolor='black'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a017f7f",
   "metadata": {},
   "source": [
    "# Wrapper to Check for MCAR, MNAR, MAR, Skewed, Multimodal, Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import normaltest\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Categorize columns by data type\n",
    "int64_cols = []\n",
    "float64_cols = []\n",
    "object_cols = []\n",
    "category_cols = []\n",
    "\n",
    "# Iterate through columns and categorize them based on dtype\n",
    "for column in data_cate.columns:\n",
    "    dtype = data_cate[column].dtype\n",
    "    if dtype == 'int64':\n",
    "        int64_cols.append(column)\n",
    "    elif dtype == 'float64':\n",
    "        float64_cols.append(column)\n",
    "    elif dtype == 'object':\n",
    "        object_cols.append(column)\n",
    "    elif dtype == 'category':\n",
    "        category_cols.append(column)\n",
    "\n",
    "# Function to check for normality using the D'Agostino and Pearson's test\n",
    "def check_normality(column_data):\n",
    "    stat, p = normaltest(column_data.dropna())\n",
    "    return p > 0.05  # p > 0.05 indicates the data is likely normal\n",
    "\n",
    "# Function to determine missing value mechanism using statistical tests\n",
    "def determine_missing_mechanism(df, column_name):\n",
    "    column = df[column_name]\n",
    "\n",
    "    # If all values are missing, the mechanism is MNAR\n",
    "    if column.isnull().sum() == column.shape[0]:\n",
    "        return \"MNAR (all values missing)\"\n",
    "\n",
    "    # Check for MCAR using Little's MCAR test (simplified version)\n",
    "    try:\n",
    "        missing_indicator = column.isnull().astype(int)\n",
    "        complete_data = df.dropna(subset=[column_name])\n",
    "        incomplete_data = df.loc[column.isnull()]\n",
    "        \n",
    "        # Perform Little's MCAR test\n",
    "        _, p_value = sm.stats.diagnostic.lilliefors(complete_data[column_name].dropna(), dist=\"norm\")\n",
    "        if p_value > 0.05:\n",
    "            return \"MCAR (missing completely at random)\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in MCAR test for column {column_name}: {e}\")\n",
    "\n",
    "    # Check for MAR using logistic regression\n",
    "    try:\n",
    "        df = df.dropna(subset=[column_name])\n",
    "        missing_indicator = column.isnull().astype(int)\n",
    "        X = df.drop([column_name], axis=1).select_dtypes(include=[\"number\"]).fillna(0)\n",
    "        y = missing_indicator[df.index]\n",
    "        \n",
    "        if len(np.unique(y)) > 1:  # Ensure the target has more than one class\n",
    "            model = LogisticRegression(max_iter=1000).fit(X, y)\n",
    "            if model.score(X, y) > 0.5:\n",
    "                return \"MAR (missing at random)\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in MAR logistic regression for column {column_name}: {e}\")\n",
    "\n",
    "    # Default to MNAR if other mechanisms cannot be established\n",
    "    return \"MNAR (missing not at random)\"\n",
    "\n",
    "# Function to analyze float64 columns with detailed statistics\n",
    "def analyze_float64_column(col, column_data):\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "\n",
    "    # Unique value counts (including NaN)\n",
    "    value_counts = column_data.value_counts(dropna=False)\n",
    "    print(f\"Unique values and counts:\\n{value_counts}\\n\")\n",
    "\n",
    "    # Count of missing values\n",
    "    missing_values = column_data.isnull().sum()\n",
    "    print(f\"Number of missing values: {missing_values}\")\n",
    "\n",
    "    if missing_values > 0:\n",
    "        # Determine missing value mechanism using statistical tests\n",
    "        missing_mechanism = determine_missing_mechanism(data_cate, col)\n",
    "        print(f\"Missing value mechanism: {missing_mechanism}\")\n",
    "\n",
    "        # Check for normality\n",
    "        is_normal = check_normality(column_data)\n",
    "        if is_normal:\n",
    "            mean = column_data.mean()\n",
    "            std = column_data.std()\n",
    "            print(f\"Data distribution: Normal\")\n",
    "            print(f\"2-standard deviation range: {mean - 2 * std} to {mean + 2 * std}\")\n",
    "        else:\n",
    "            q25 = column_data.quantile(0.25)\n",
    "            q75 = column_data.quantile(0.75)\n",
    "            print(f\"Data distribution: Not Normal\")\n",
    "            print(f\"Interquartile range (IQR): {q25} to {q75}\")\n",
    "\n",
    "        # Determine skewness and multimodality\n",
    "        skewness = column_data.skew()\n",
    "        multimodal = len(column_data.value_counts()) > 2  # Simplistic check\n",
    "        print(f\"Skewness: {'Yes' if abs(skewness) > 1 else 'No'}\")\n",
    "        print(f\"Multimodal: {'Yes' if multimodal else 'No'}\")\n",
    "\n",
    "        # Check for outliers using 1.5 * IQR rule\n",
    "        iqr = column_data.quantile(0.75) - column_data.quantile(0.25)\n",
    "        lower_bound = column_data.quantile(0.25) - 1.5 * iqr\n",
    "        upper_bound = column_data.quantile(0.75) + 1.5 * iqr\n",
    "        outliers = column_data[(column_data < lower_bound) | (column_data > upper_bound)]\n",
    "        print(f\"Outliers: {'Yes' if not outliers.empty else 'No'}\")\n",
    "\n",
    "    # Range of values\n",
    "    min_value = column_data.min(skipna=True)\n",
    "    max_value = column_data.max(skipna=True)\n",
    "    print(f\"Range of values: {min_value} to {max_value}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Function to analyze all columns in a category\n",
    "def analyze_columns(columns, dtype_name):\n",
    "    print(f\"\\n{'='*10} Analyzing {dtype_name} Columns {'='*10}\\n\")\n",
    "    for col in columns:\n",
    "        if dtype_name == \"float64\" and data_cate[col].isnull().sum() > 0:\n",
    "            analyze_float64_column(col, data_cate[col])\n",
    "        else:\n",
    "            # Default analysis for other types\n",
    "            print(f\"Column: {col}\")\n",
    "\n",
    "            # Unique value counts (including NaN)\n",
    "            value_counts = data_cate[col].value_counts(dropna=False)\n",
    "            print(f\"Unique values and counts:\\n{value_counts}\\n\")\n",
    "\n",
    "            # Count of missing values\n",
    "            missing_values = data_cate[col].isnull().sum()\n",
    "            print(f\"Number of missing values: {missing_values}\")\n",
    "\n",
    "            # Range for numeric columns\n",
    "            if dtype_name in [\"int64\", \"float64\"]:\n",
    "                min_value = data_cate[col].min(skipna=True)\n",
    "                max_value = data_cate[col].max(skipna=True)\n",
    "                print(f\"Range of values: {min_value} to {max_value}\")\n",
    "\n",
    "            print(\"-\" * 40)\n",
    "    print(f\"\\n{'='*30}\\n\")\n",
    "\n",
    "# Analyze each category\n",
    "analyze_columns(int64_cols, \"int64\")\n",
    "analyze_columns(float64_cols, \"float64\")\n",
    "analyze_columns(object_cols, \"object\")\n",
    "analyze_columns(category_cols, \"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ecc3a",
   "metadata": {},
   "source": [
    "Wrapper for Impuatation Analysis with Statistical Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ad2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import normaltest, pearsonr\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Helper Functions\n",
    "\n",
    "# Function to determine sparsity\n",
    "def determine_sparsity(column_data):\n",
    "    unique_values = column_data.dropna().unique()\n",
    "    sparsity_ratio = (len(unique_values) / len(column_data))\n",
    "    return \"Sparse\" if sparsity_ratio < 0.1 else \"Not Sparse\"\n",
    "\n",
    "# Function to check for missingness patterns\n",
    "def check_missingness_pattern(data, col):\n",
    "    missing = data[col].isnull().astype(int)\n",
    "    patterns = {}\n",
    "    for other_col in data.columns:\n",
    "        if other_col != col:\n",
    "            correlation = missing.corr(data[other_col].notnull().astype(int))\n",
    "            patterns[other_col] = correlation\n",
    "    correlated_columns = [k for k, v in patterns.items() if abs(v) > 0.5]\n",
    "    return \"Dependent\" if correlated_columns else \"Independent\"\n",
    "\n",
    "# Function to analyze feature relationships (correlation for numeric)\n",
    "def check_feature_relationship(data, col):\n",
    "    column_data = data[col]\n",
    "    correlations = {}\n",
    "    for other_col in data.columns:\n",
    "        if other_col != col:\n",
    "            if column_data.dtype in [\"float64\", \"int64\"] and data[other_col].dtype in [\"float64\", \"int64\"]:\n",
    "                # Align indices to handle missing values\n",
    "                aligned_data = pd.concat([column_data, data[other_col]], axis=1).dropna()\n",
    "                if len(aligned_data) > 1:  # Ensure there are at least 2 data points\n",
    "                    corr, _ = pearsonr(aligned_data.iloc[:, 0], aligned_data.iloc[:, 1])\n",
    "                    correlations[other_col] = corr\n",
    "    return \"High Correlation\" if any(abs(v) > 0.7 for v in correlations.values()) else \"Low Correlation\"\n",
    "\n",
    "# Function to check skewness\n",
    "def check_skewness(column_data):\n",
    "    skewness = column_data.skew()\n",
    "    return \"Yes\" if abs(skewness) > 1 else \"No\"\n",
    "\n",
    "# Function to check for outliers using IQR\n",
    "def check_outliers(column_data):\n",
    "    iqr = column_data.quantile(0.75) - column_data.quantile(0.25)\n",
    "    lower_bound = column_data.quantile(0.25) - 1.5 * iqr\n",
    "    upper_bound = column_data.quantile(0.75) + 1.5 * iqr\n",
    "    outliers = column_data[(column_data < lower_bound) | (column_data > upper_bound)]\n",
    "    return \"Yes\" if not outliers.empty else \"No\"\n",
    "\n",
    "# Function to check for normality using the D'Agostino and Pearson's test\n",
    "def check_normality(column_data):\n",
    "    stat, p = normaltest(column_data.dropna())\n",
    "    return p > 0.05  # p > 0.05 indicates the data is likely normal\n",
    "\n",
    "# 2. Analysis Functions for Each Data Type\n",
    "\n",
    "categorical_data_mode_imputation = []\n",
    "categorical_data_multivariate_imputation = []\n",
    "\n",
    "def analyze_float64_column(col, column_data, data):\n",
    "    print(f\"\\nColumn: {col} (float64)\")\n",
    "\n",
    "    # General Stats\n",
    "    missing_values = column_data.isnull().sum()\n",
    "    print(f\"Missing values: {missing_values}\")\n",
    "\n",
    "    if missing_values > 0:\n",
    "        # Sparsity\n",
    "        sparsity = determine_sparsity(column_data)\n",
    "        print(f\"Sparsity: {sparsity}\")\n",
    "\n",
    "        # Missingness Pattern\n",
    "        missingness_pattern = check_missingness_pattern(data, col)\n",
    "        print(f\"Missingness Pattern: {missingness_pattern}\")\n",
    "\n",
    "        # Feature Relationship\n",
    "        feature_relationship = check_feature_relationship(data, col)\n",
    "        print(f\"Relationship with other features: {feature_relationship}\")\n",
    "\n",
    "        # Skewness and Outliers\n",
    "        skewed = check_skewness(column_data)\n",
    "        outliers = check_outliers(column_data)\n",
    "        print(f\"Skewed: {skewed}\")\n",
    "        print(f\"Outliers: {outliers}\")\n",
    "\n",
    "    # Check for normality\n",
    "    is_normal = check_normality(column_data)\n",
    "    if is_normal:\n",
    "        mean = column_data.mean()\n",
    "        std = column_data.std()\n",
    "        print(f\"Data distribution: Normal\")\n",
    "        print(f\"2-standard deviation range: {mean - 2 * std} to {mean + 2 * std}\")\n",
    "        imputation_recommendation = \"Mean\"\n",
    "    else:\n",
    "        q25 = column_data.quantile(0.25)\n",
    "        q75 = column_data.quantile(0.75)\n",
    "        print(f\"Data distribution: Not Normal\")\n",
    "        print(f\"Interquartile range (IQR): {q25} to {q75}\")\n",
    "        imputation_recommendation = \"Median\"\n",
    "\n",
    "    # Imputation Recommendation based on table\n",
    "    print(f\"Imputation Recommendation: {imputation_recommendation}\")\n",
    "\n",
    "def analyze_int64_column(col, column_data, data):\n",
    "    print(f\"\\nColumn: {col} (int64)\")\n",
    "\n",
    "    # General Stats\n",
    "    missing_values = column_data.isnull().sum()\n",
    "    print(f\"Missing values: {missing_values}\")\n",
    "\n",
    "    if missing_values > 0:\n",
    "        # Sparsity\n",
    "        sparsity = determine_sparsity(column_data)\n",
    "        print(f\"Sparsity: {sparsity}\")\n",
    "\n",
    "        # Missingness Pattern\n",
    "        missingness_pattern = check_missingness_pattern(data, col)\n",
    "        print(f\"Missingness Pattern: {missingness_pattern}\")\n",
    "\n",
    "        # Feature Relationship\n",
    "        feature_relationship = check_feature_relationship(data, col)\n",
    "        print(f\"Relationship with other features: {feature_relationship}\")\n",
    "\n",
    "    # Imputation Recommendation\n",
    "    print(\"Imputation Recommendation: Median\")\n",
    "\n",
    "def analyze_categorical_column(col, column_data, data):\n",
    "    print(f\"\\nColumn: {col} (object/category)\")\n",
    "\n",
    "    # General Stats\n",
    "    missing_values = column_data.isnull().sum()\n",
    "    print(f\"Missing values: {missing_values}\")\n",
    "\n",
    "    if missing_values > 0:\n",
    "        # Sparsity\n",
    "        sparsity = determine_sparsity(column_data)\n",
    "        print(f\"Sparsity: {sparsity}\")\n",
    "\n",
    "        # Missingness Pattern\n",
    "        missingness_pattern = check_missingness_pattern(data, col)\n",
    "        print(f\"Missingness Pattern: {missingness_pattern}\")\n",
    "\n",
    "        # Mode and Cardinality\n",
    "        mode = column_data.mode().iloc[0] if not column_data.mode().empty else \"N/A\"\n",
    "        cardinality = column_data.nunique()\n",
    "        print(f\"Mode: {mode}, Cardinality: {cardinality}\")\n",
    "\n",
    "        # Imputation Recommendation based on table\n",
    "        if missingness_pattern == \"Dependent\" and sparsity == \"Sparse\":\n",
    "            if cardinality == 2:\n",
    "                imputation_recommendation = \"KNN Imputation or Multiple Imputation\"\n",
    "                categorical_data_multivariate_imputation.append(col)\n",
    "            else:\n",
    "                imputation_recommendation = \"Mode\"\n",
    "                categorical_data_mode_imputation.append(col)\n",
    "        else:\n",
    "            imputation_recommendation = \"Mode\"\n",
    "            categorical_data_mode_imputation.append(col)\n",
    "    else:\n",
    "        imputation_recommendation = \"Mode\"\n",
    "        categorical_data_mode_imputation.append(col)\n",
    "\n",
    "    # Imputation Recommendation\n",
    "    print(f\"Imputation Recommendation: {imputation_recommendation}\")\n",
    "\n",
    "# 3. Full Analysis Wrapper\n",
    "\n",
    "def analyze_dataset(data):\n",
    "    print(\"Starting analysis...\\n\")\n",
    "\n",
    "    int64_cols = [col for col in data.columns if data[col].dtype == \"int64\"]\n",
    "    float64_cols = [col for col in data.columns if data[col].dtype == \"float64\"]\n",
    "    object_cols = [col for col in data.columns if data[col].dtype == \"object\"]\n",
    "    category_cols = [col for col in data.columns if data[col].dtype == \"category\"]\n",
    "\n",
    "    print(\"------ Int64 dtype -----------\\n\")\n",
    "    for col in int64_cols:\n",
    "        analyze_int64_column(col, data[col], data)\n",
    "\n",
    "    print(\"\\n------ Float64 dtype -----------\\n\")\n",
    "    for col in float64_cols:\n",
    "        analyze_float64_column(col, data[col], data)\n",
    "\n",
    "    print(\"\\n------ Object/Category dtype -----------\\n\")\n",
    "    for col in object_cols + category_cols:\n",
    "        analyze_categorical_column(col, data[col], data)\n",
    "\n",
    "    print(\"Analysis completed.\")\n",
    "    print(\"\\nCategorical Data Mode Imputation Columns:\", categorical_data_mode_imputation)\n",
    "    print(\"Categorical Data Multivariate Imputation Columns:\", categorical_data_multivariate_imputation)\n",
    "\n",
    "# Example usage (Replace `data` with your dataframe):\n",
    "analyze_dataset(data_cate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cd99f",
   "metadata": {},
   "source": [
    "Train-test split before Imputation Implemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d535b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_cate.drop(\"asthma_combined\", axis=1),  # features\n",
    "    data_cate[\"asthma_combined\"],               # target\n",
    "    test_size=0.3,                              # test set proportion\n",
    "    random_state=0,                             # reproducibility\n",
    "    stratify=data_cate[\"asthma_combined\"]       # stratify by target\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afbcb9",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e18d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Load the lists back\n",
    "with open(\"imputation_lists.json\", \"r\") as f:\n",
    "    imputation_lists = json.load(f)\n",
    "\n",
    "categorical_data_mode_imputation = imputation_lists[\"categorical_data_mode_imputation\"]\n",
    "categorical_data_multivariate_imputation = imputation_lists[\"categorical_data_multivariate_imputation\"]\n",
    "float_columns_to_impute = imputation_lists[\"float_columns_to_impute\"]\n",
    "\n",
    "print(\"Loaded lists:\", categorical_data_mode_imputation, categorical_data_multivariate_imputation, float_columns_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6ebda",
   "metadata": {},
   "source": [
    "Global Asthma Network pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ae8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# for feature engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine import imputation as mdi\n",
    "from feature_engine import discretisation as dsc\n",
    "from feature_engine import encoding as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing \"asthma_combined\" from the \"categorical_data_multivariate_imputation\" list as \"asthma_combined\" is the target column.\n",
    "\n",
    "# Remove 'asthma_combined' from the list\n",
    "categorical_data_multivariate_imputation.remove(\"asthma_combined\")\n",
    "\n",
    "# Print the updated list\n",
    "print(categorical_data_multivariate_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_pipe = Pipeline(    [\n",
    "        (\n",
    "            \"median_imputer\",\n",
    "            mdi.MeanMedianImputer(\n",
    "                imputation_method=\"median\", variables=float_columns_to_impute\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            'categorical_imputer',\n",
    "             mdi.CategoricalImputer(\n",
    "                 imputation_method='frequent',\n",
    "                 variables=categorical_data_mode_imputation\n",
    "             ),\n",
    "        ),\n",
    "\n",
    "        (\n",
    "            'arbitrary_categorical_imputer',\n",
    "             mdi.CategoricalImputer(\n",
    "                 imputation_method='missing',\n",
    "                 fill_value='Missing',\n",
    "                 variables=categorical_data_multivariate_imputation\n",
    "             ),\n",
    "        ),        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# source = https://feature-engine.trainindata.com/en/1.8.x/api_doc/imputation/CategoricalImputer.html#feature_engine.imputation.CategoricalImputer\n",
    "# source = https://feature-engine.trainindata.com/en/1.8.x/api_doc/imputation/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find the learned parametes\n",
    "gan_pipe.named_steps[\"median_imputer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aec071",
   "metadata": {},
   "source": [
    "Fit the imputation pipeline to Train features (Target excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_pipe.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c05441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data sets\n",
    "\n",
    "X_train_impu = gan_pipe.transform(X_train)\n",
    "X_test_impu = gan_pipe.transform(X_test)\n",
    "\n",
    "X_train_impu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving DataFrames as .pkl Files\n",
    "\n",
    "# Save DataFrames and Series to pickle files\n",
    "X_train_impu.to_pickle('X_train_impu70.pkl')\n",
    "X_test_impu.to_pickle('X_test_impu30.pkl')\n",
    "\n",
    "print(\"Imputed Data saved as .pkl files\")\n",
    "print(X_train_impu.shape, X_test_impu.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62081b60",
   "metadata": {},
   "source": [
    "Drop these columns since they are 98% missing:\n",
    "school_india, pincode_india, pincode_poll, latitude_poll, longitude_poll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5d6fa",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988071a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle files\n",
    "import pandas as pd\n",
    "X_train_impu = pd.read_pickle('70-30(imputed)/X_train_impu70.pkl')\n",
    "X_test_impu = pd.read_pickle('70-30(imputed)/X_test_impu30.pkl')\n",
    "y_train = pd.read_pickle('70-30(imputed)/y_train70.pkl')\n",
    "y_test = pd.read_pickle('70-30(imputed)/y_test30.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfcaac6",
   "metadata": {},
   "source": [
    "We must encode the categorical data attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = X_train_impu.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Check unique values in each categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"Column: {col}, dtype: {X_train_impu[col].dtype},   Unique Values: {data_enc[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d640791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking each datatype\n",
    "\n",
    "#medpill , medpil1b, medpil2b, medpil3b, medpil4b are not in object column. lets check\n",
    "\n",
    "# lets categorize each of the column intot the above datatype\n",
    "\n",
    "# Initialize empty lists for each data type\n",
    "int64_cols = []\n",
    "float64_cols = []\n",
    "object_cols = []\n",
    "int32_cols = []\n",
    "category_cols = []\n",
    "float32_cols = []\n",
    "int16_cols = []\n",
    "float16_cols = []\n",
    "\n",
    "# Iterate through columns and categorize them based on dtype\n",
    "for column in X_train_impu.columns:\n",
    "    dtype = X_train_impu[column].dtype\n",
    "    if dtype == 'int64':\n",
    "        int64_cols.append(column)\n",
    "    elif dtype == 'float64':\n",
    "        float64_cols.append(column)\n",
    "    elif dtype == 'object':\n",
    "        object_cols.append(column)\n",
    "    elif dtype == 'bool':\n",
    "        bool_cols.append(column)\n",
    "    elif dtype == 'datetime64[ns]':\n",
    "        datetime64_cols.append(column)\n",
    "    elif dtype == 'timedelta64[ns]':\n",
    "        timedelta_cols.append(column)\n",
    "    elif dtype == 'category':\n",
    "        category_cols.append(column)\n",
    "    elif dtype == 'int32':\n",
    "        int32_cols.append(column)\n",
    "    elif dtype == 'float32':\n",
    "        float32_cols.append(column)\n",
    "    elif dtype == 'int16':\n",
    "        int16_cols.append(column)\n",
    "    elif dtype == 'float16':\n",
    "        float16_cols.append(column)\n",
    "    elif dtype == 'uint8':\n",
    "        uint8_cols.append(column)\n",
    "    elif dtype == 'uint16':\n",
    "        uint16_cols.append(column)\n",
    "    elif dtype == 'uint32':\n",
    "        uint32_cols.append(column)\n",
    "    elif dtype == 'uint64':\n",
    "        uint64_cols.append(column)\n",
    "\n",
    "# Print the columns categorized into each data type\n",
    "print(\"int64_cols:\", int64_cols)\n",
    "print(\"\\nfloat64_cols:\", float64_cols)\n",
    "print(\"\\nobject_cols:\", object_cols)\n",
    "\n",
    "# print(\"\\nbool_cols:\", bool_cols)\n",
    "# print(\"\\ndatetime64_cols:\", datetime64_cols)\n",
    "# print(\"\\ntimedelta_cols:\", timedelta_cols)\n",
    "\n",
    "print(\"\\ncategory_cols:\", category_cols)\n",
    "print(\"\\n int32_cols:\", int32_cols)\n",
    "print(\"\\n float32_cols:\", float32_cols)\n",
    "print(\"\\n int16_cols:\", int16_cols)\n",
    "print(\"\\n float16_cols:\", float16_cols)\n",
    "# print(\"\\n uint8_cols:\", uint8_cols)\n",
    "# print(\"\\n uint16_cols:\", uint16_cols)\n",
    "# print(\"\\n uint32_cols:\", uint32_cols)\n",
    "# print(\"\\n uint64_cols:\", uint64_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a11680",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = category_cols + object_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f3602",
   "metadata": {},
   "source": [
    "Extracted this encoding recommendation from the propriority wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd492f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_recommendations={'OneHotEncoding': ['milkynga', 'medpil1b', 'medpil2b', 'medpil3b', 'medpil4b'], \n",
    "                          'LabelEncoding': ['sex', 'whezev', 'whezage', 'whez12', 'nwhez12', 'awake12', 'speech12', \n",
    "                            'medpuff', 'sabafreq', 'labafreq', 'icsfreq', 'combfreq', 'medpill', 'docbrt12', 'erbrth12', \n",
    "                            'hosbrt12', 'exwhez12', 'cough12', 'pnoseev', 'pnoseage', 'pnose12', 'iitch12', 'ieyes12', 'iactiv12', \n",
    "                            'hfeverev', 'hfevdoc', 'rashev', 'mparaprg', 'msmokprg', 'mpcar01', 'mpcar02', 'mpcar03', 'mpcar04', \n",
    "                            'chprem', 'brstfed', 'nbrstfed', 'nbrstexc', 'parayng', 'nchstyng', 'antibiot', 'nantibiot', 'antibioch', 'sheepyng', \n",
    "                            'catyng', 'dogyng', 'aniyng', 'wheezyng', 'medyng', 'medyng1', 'medyng2', 'medyng3', 'medyng4', 'medyng5', 'medyng6', 'medyng7', \n",
    "                            'chcaryng', 'chcarold', 'exercise', 'televis', 'computer', 'pneumon', 'twin', 'cntrybir', 'chhmchng', 'trucfreq', 'meat',\n",
    "                            'seafood', 'fruit', 'vegecook', 'vegeraw', 'pulses', 'cereals', 'bread', 'pasta', 'rice', 'margarin', 'butter', 'oliveoil', \n",
    "                            'milk', 'dairyoth', 'eggs', 'nuts', 'potato', 'sugar', 'burger', 'fastfood', 'softdrnk', 'catnow', 'dognow', 'paranow', \n",
    "                            'chflr01_1', 'chflr01_2', 'chflr01_3', 'chflr01_4', 'chflr02_1', 'chflr02_2', 'chflr02_3', 'chflr02_4', 'chflr03_1', 'chflr03_2', \n",
    "                            'chflr03_3', 'chflr03_4', 'chflr04_1', 'chflr04_2', 'chflr04_3', 'chflr04_4', 'nebumed_india', 'nebulasthalevo_india', 'nebuduo_india', \n",
    "                            'nebubude_india', 'banana_india', 'guava_india', 'curdyog_india', 'colddrinks_india', 'icecreams_india', 'cakepastries_india', \n",
    "                            'packcrunchy_india', 'medpil1a', 'medpil2a', 'medpil3a', 'medpil4a', 'cbiroth'], \n",
    "                          'BinaryEncoding': [], 'TargetEncoding': [], 'CountEncoding': [], 'UnrecognizedColumns': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6305408",
   "metadata": {},
   "source": [
    "Encoding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Extract specific recommendations\n",
    "one_hot_recommendation = encoding_recommendations['OneHotEncoding']\n",
    "label_recommendation = encoding_recommendations['LabelEncoding']\n",
    "\n",
    "# Define the pipeline\n",
    "gan_encd_pipe = Pipeline(\n",
    "    [\n",
    "        (\"ohe\", OneHotEncoder(variables=one_hot_recommendation, drop_last=True)),\n",
    "        (\"labelencoding\", OrdinalEncoder(encoding_method=\"arbitrary\", variables=label_recommendation)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the pipeline (example usage)\n",
    "gan_encd_pipe.fit(X_train_impu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7dc819",
   "metadata": {},
   "source": [
    "Checking the datatypes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80559abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to categorize columns by data type\n",
    "dtype_categories = {\n",
    "    'int64': [],\n",
    "    'float64': [],\n",
    "    'object': [],\n",
    "    'bool': [],\n",
    "    'datetime64[ns]': [],\n",
    "    'timedelta64[ns]': [],\n",
    "    'category': [],\n",
    "    'int32': [],\n",
    "    'float32': [],\n",
    "    'int16': [],\n",
    "    'float16': [],\n",
    "    'uint8': [],\n",
    "    'uint16': [],\n",
    "    'uint32': [],\n",
    "    'uint64': []\n",
    "}\n",
    "\n",
    "# Iterate through columns and categorize them based on dtype\n",
    "for column in X_train_enc.columns:\n",
    "    dtype = str(X_train_enc[column].dtype)\n",
    "    if dtype in dtype_categories:\n",
    "        dtype_categories[dtype].append(column)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{column}' has an unexpected dtype '{dtype}'\")\n",
    "\n",
    "# Print categorized columns\n",
    "for dtype, columns in dtype_categories.items():\n",
    "    print(f\"{dtype}: {columns if columns else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e0273",
   "metadata": {},
   "source": [
    "# END OF FEATURE ENGINEERING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
